{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5807914,"sourceType":"datasetVersion","datasetId":3322159}],"dockerImageVersionId":30497,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/colonpolyp'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To examine the relationship between numerical and categorical data, you can use various statistical methods and visualization tools. Here is an example code to summarize and analyze both types of data:\n\n1. Numerical Data Summary:\n   - Calculate summary statistics (mean, standard deviation, etc.) for numerical variables such as age, Ki67, VEGF, and CD34.\n\n2. Categorical Data Summary:\n   - Summarize categorical variables like gender, location, type, and subtype using descriptive statistics, including the count, unique categories, and top frequency.\n\n3. Correlation Analysis:\n   - Compute the correlation matrix to evaluate the relationships between numerical variables. This matrix shows the pairwise correlation coefficients.\n\n4. Visualization:\n   - Generate a heatmap using seaborn library to visually represent the correlation matrix. This heatmap provides a color-coded representation of the strength and direction of the relationships between numerical variables.\n\nBy analyzing the numerical and categorical data together, you can gain insights into their relationships and identify any significant correlations. The summary tables and visualization will help you understand the patterns and associations within your dataset.\n\nNote: Ensure that the necessary Python libraries (such as pandas, seaborn, and matplotlib) are installed before running the code.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Specify the path to the Excel file containing IHC values\n\n# Read the Excel file\nihc_data = pd.read_excel('/kaggle/input/colonpolyp/ihc_data.xlsx')\n\n\n# Perform desired operations to analyze the IHC values\n# For example, you can calculate statistical summaries or examine the relationship between markers\n\n# You can use the print() function to display the results of your operations\nprint(ihc_data.head())  # Display the first few rows of the data\n","metadata":{"execution":{"iopub.status.busy":"2023-05-26T18:06:42.813945Z","iopub.execute_input":"2023-05-26T18:06:42.814388Z","iopub.status.idle":"2023-05-26T18:06:43.570773Z","shell.execute_reply.started":"2023-05-26T18:06:42.814353Z","shell.execute_reply":"2023-05-26T18:06:43.569633Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The ID values in this dataset serve as unique identifiers for both colonoscopy videos and histopathology images. Each ID value represents the same patient, indicating that videos and histopathology images with the same ID belong to the same individual.\n\nBy using the corresponding ID value, you can access the colonoscopy video of a specific patient and find the histopathology images with the same ID. This allows you to compare or correlate the colonoscopy findings with the histopathology evaluations for individual patients.\n\nThis linkage between ID values, colonoscopy videos, and histopathology images enables you to analyze and understand the relationship between endoscopic observations and histological diagnoses in the context of each patient.","metadata":{}},{"cell_type":"markdown","source":"In the above code, you need to specify the path to the Excel file containing the IHC values by assigning it to the ihc_file variable. Then, you can use the pd.read_excel() function to read the Excel file. After that, you can perform the desired operations on the ihc_data variable to analyze the IHC values.\nFor example, I have included the print(ihc_data.head()) code to display the first few rows of the data. You can add your own operations below this code to analyze the IHC values further.\nPlease make sure to modify the code according to your specific requirements and ensure that you have installed the necessary libraries (we are using the pandas library). Also, ensure that the Excel file containing the IHC values is properly formatted.","metadata":{}},{"cell_type":"code","source":"print(ihc_data.columns)\n# renamed columns names\nihc_data = ihc_data.rename(columns={\n    'Ki-67(clone30-9)': 'Ki67',\n    'BRAF(cloneV600E)': 'BRAF',\n    'PD-L1epithelium(clone SP142)': 'PDL1epith',\n    'PD-L1lymphocyte(clone SP142)': 'PDL1lymph',\n    'VEGF(clone SP125)': 'VEGF',\n    'CD34(cloneQBend/10)': 'CD34',\n    'CD34(cloneQBend/10)skor': 'CD34skor',\n    'p53(clonebp53-11)': 'p53'\n})\nprint(ihc_data.columns)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T18:08:32.928568Z","iopub.execute_input":"2023-05-26T18:08:32.929299Z","iopub.status.idle":"2023-05-26T18:08:32.941817Z","shell.execute_reply.started":"2023-05-26T18:08:32.929252Z","shell.execute_reply":"2023-05-26T18:08:32.940577Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numeric_columns = ['age', 'Ki67', 'VEGF', 'CD34', 'CD34skor', 'p53']\ncategorical_columns = ['gender', 'location', 'type', 'subtype', 'BRAF', 'PDL1epith', 'PDL1lymph']\n\n# Summarize the numerical data\nnumerical_summary = ihc_data[numeric_columns].describe()\n\n# Summarize the categorical data\ncategorical_summary = ihc_data[categorical_columns].describe(include=['O'])\n\n# Print the summary tables\nprint(\"Summary of Numerical Data:\")\nprint(numerical_summary)\nprint(\"\\nSummary of Categorical Data:\")\nprint(categorical_summary)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T18:09:16.574806Z","iopub.execute_input":"2023-05-26T18:09:16.575216Z","iopub.status.idle":"2023-05-26T18:09:16.631683Z","shell.execute_reply.started":"2023-05-26T18:09:16.575176Z","shell.execute_reply":"2023-05-26T18:09:16.630234Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The given code is used to summarize the data in the specified numeric and categorical columns.\n\nFor the numeric columns (age, Ki67, VEGF, CD34, CD34skor, p53), the code calculates summary statistics including count, mean, standard deviation, minimum value, 25th percentile, median (50th percentile), 75th percentile, and maximum value. This provides an overview of the distribution and central tendency of the numeric variables.\n\nFor the categorical columns (gender, location, type, subtype, BRAF, PDL1epith, PDL1lymph), the code calculates summary statistics including count, unique values, top (most frequent) value, and frequency of the top value. This provides information about the distribution and frequency of different categories within each categorical variable.\n\nThe code then prints the summary tables for both the numerical and categorical data, allowing for a quick overview of the data distribution and characteristics in each column.","metadata":{}},{"cell_type":"markdown","source":" Examine your dataset and identify which variables are normally distributed.\n\nFor normality check, you can use statistical tests such as Shapiro-Wilk test or Kolmogorov-Smirnov test. These tests help you determine whether a variable follows a normal distribution. The null hypothesis (H0) states that the variable is normally distributed.\n\nThe Shapiro-Wilk test cannot be applied when there are missing values (NaN) in the data. To handle this, we can remove the missing values from the data before performing the Shapiro-Wilk test.\n\nExample Code (Shapiro-Wilk test):","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import shapiro\n\n# Replace non-finite values with NaN in 'age' column\nihc_data['age'] = pd.to_numeric(ihc_data['age'], errors='coerce')\n\n# Convert 'age' column to integer type, handling NaN values\nihc_data['age'] = ihc_data['age'].astype('Int64', errors='ignore')\n\n# Iterate over each numerical variable\nfor column in numerical_summary.columns:\n    data = ihc_data[column].dropna()  # Remove missing values\n    \n    # Apply Shapiro-Wilk test\n    stat, p = shapiro(data)\n    \n    # Print the results\n    print(\"Variable:\", column)\n    print(\"Test Statistic:\", stat)\n    print(\"p-value:\", p)\n    \n    if p > 0.05:\n        print(\"The data follows a normal distribution.\")\n    else:\n        print(\"The data does not follow a normal distribution.\")\n    \n    # Plot histogram\n    plt.figure(figsize=(8, 6))\n    plt.hist(data, bins='auto', alpha=0.7)\n    plt.xlabel(column)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Histogram of \" + column)\n    plt.show()\n    \n    print()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-26T18:56:45.138110Z","iopub.execute_input":"2023-05-26T18:56:45.138539Z","iopub.status.idle":"2023-05-26T18:56:46.999626Z","shell.execute_reply.started":"2023-05-26T18:56:45.138494Z","shell.execute_reply":"2023-05-26T18:56:46.997701Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To assess the distribution of categorical variables, we can use frequency tables or bar plots to visualize the distribution of each category.\n\nHere's an example code to create frequency tables for each categorical variable:","metadata":{}},{"cell_type":"code","source":"# Iterate over each categorical variable\nfor column in categorical_summary.columns:\n    data = ihc_data[column]  # Select the column data\n    \n    # Create frequency table\n    frequency_table = data.value_counts().reset_index()\n    frequency_table.columns = ['Category', 'Frequency']\n    \n    # Print the frequency table\n    print(\"Variable:\", column)\n    print(frequency_table)\n    print()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-26T18:58:53.883985Z","iopub.execute_input":"2023-05-26T18:58:53.884420Z","iopub.status.idle":"2023-05-26T18:58:53.917136Z","shell.execute_reply.started":"2023-05-26T18:58:53.884388Z","shell.execute_reply":"2023-05-26T18:58:53.913955Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Iterate over each categorical column\nfor column in categorical_columns:\n    plt.figure(figsize=(10, 6))\n    sns.countplot(data=ihc_data, x=column)\n    plt.title(column)\n    plt.xlabel(\"Categories\")\n    plt.ylabel(\"Count\")\n    plt.xticks(rotation=45)\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-26T19:34:17.176507Z","iopub.execute_input":"2023-05-26T19:34:17.176985Z","iopub.status.idle":"2023-05-26T19:34:19.320119Z","shell.execute_reply.started":"2023-05-26T19:34:17.176952Z","shell.execute_reply":"2023-05-26T19:34:19.318893Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Elbette! İstatistiksel analizler yaparak verileriniz arasındaki anlamlı ilişkileri belirleyebilir ve uygun bir model seçebilirsiniz. Hangi modelin en uygun olduğunu belirlemek için aşağıdaki adımları takip edebilirsiniz:\n\n1. Veri setinizi gözden geçirin ve verilerinizin yapısını anlayın.\n2. İlgilenen değişkenler arasındaki ilişkiyi belirlemek için korelasyon analizi yapın. Pearson veya Spearman korelasyon katsayılarını kullanarak ilişkileri değerlendirebilirsiniz.\n3. Verilerinizin normal dağılımını kontrol edin. Normal dağılıma uygun olan veriler için parametrik istatistiksel analizler (t-test, ANOVA, vb.) kullanabilirsiniz. Normal dağılıma uymayan veriler için ise non-parametrik analizler (Wilcoxon testi, Kruskal-Wallis testi, vb.) tercih edebilirsiniz.\n4. Veri setinizdeki gruplar arasındaki farkları veya ilişkileri belirlemek için uygun istatistiksel testleri uygulayın.\n5. Model seçimi için veri setinizin özelliklerine göre uygun bir analiz yöntemi belirleyin. Regresyon analizi, lojistik regresyon, ANOVA, doğrusal veya lojistik karar ağaçları gibi modellerden birini kullanabilirsiniz.\n6. Modelinizi değerlendirmek için performans ölçütlerini kullanın. Bu ölçütler arasında R-kare, AIC, BIC, confusion matrix, doğruluk, hassasiyet, özgüllük gibi metrikler bulunabilir.\n7. Sonuçları yorumlayın ve araştırma amacınıza uygun çıkarımlar yapın.\n\nKod yazmadan önce hangi analizleri yapmak istediğinizi ve hangi modeli seçmek istediğinizi daha ayrıntılı olarak belirtir misiniz? Böylece size daha spesifik bir kod verebilirim.","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Yes, you can perform a normality check for categorical variables using different statistical tests. One common test for categorical variables is the Chi-square test of independence. This test determines whether there is a significant association between two categorical variables.\n\nHere's an example code to perform a Chi-square test for each categorical variable in your dataset:\n\n```python\nfrom scipy.stats import chi2_contingency\n\n# Iterate over each categorical variable\nfor column in categorical_summary.columns:\n    contingency_table = pd.crosstab(ihc_data[column], ihc_data['pathologic_diagnosis'])  # Create a contingency table\n    \n    # Apply Chi-square test\n    chi2, p, _, _ = chi2_contingency(contingency_table)\n    \n    # Print the results\n    print(\"Variable:\", column)\n    print(\"Chi-square statistic:\", chi2)\n    print(\"p-value:\", p)\n    \n    if p > 0.05:\n        print(\"There is no significant association between the variables.\")\n    else:\n        print(\"There is a significant association between the variables.\")\n    \n    print()\n```\n\nThis code will iterate over each categorical variable in your dataset, create a contingency table between that variable and the 'pathologic_diagnosis' variable, perform the Chi-square test, and print the test statistic and p-value. Based on the p-values, you can determine whether each variable is significantly associated with the 'pathologic_diagnosis' variable or not.","metadata":{},"outputs":[],"execution_count":null}]}